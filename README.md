ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§

Get ready for a moonshot journey as we work here on crafting an AI buddy to enhance your English skills. Let's reach for the stars together!

ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§ ğŸ‡¬ğŸ‡§

==============================

## Services

The project consists of the following services:

1. **Frontend:** A React application that provides the user interface for interacting with the AI buddy.

2. **Postgres:** A Postgres server acting as a main database for storing users information in accordance with ACID and, it's still up in the air, probably as a database for storing vector embeddings via pgvector.

3. **Service2:** A Python (FastAPI)-based backend that coordinates the communication between the frontend and Service3. This service also manages the interaction history between the user and the chatbot.

4. **Service3:** Another Python (FastAPI)-based backend hosting the chatbot algorithm. This service communicates with the AI engine (OpenAI GPT-3.5-turbo/GPT-4 in the first stage of the project and something open-source in the next one) to process user queries and generate suitable responses.

5. **Redis:** A Redis server used for state storage across the services.


## Setup

To get the project up and running, make sure Docker is installed on your system.

Then, run the following command:

```bash
docker-compose up
```

This command starts all services using the `docker-compose.yml` file. It downloads the necessary Docker images, creates associated containers, and gets them running together.

## Env Variable

Rename the .env.example to .env and set your OpenAI API Key.

## Data Population
The provided insert_data.py script can be used to populate the Postgres database with your (currently mine) data. To do this, run the script once the services are up and running. It will connect to the Postgres service, create the necessary tables, and insert data into them.

Project Organization
------------

    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ Makefile            <- Makefile with commands like `make data` or `make train`
    â”œâ”€â”€ README.md           <- The top-level README for developers using this project.
    â”œâ”€â”€ data
    â”‚Â Â  â”œâ”€â”€ external        <- Data from third party sources.
    â”‚Â Â  â”œâ”€â”€ interim         <- Intermediate data that has been transformed.
    â”‚Â Â  â”œâ”€â”€ processed       <- The final, canonical data sets for modeling.
    â”‚Â Â  â””â”€â”€ raw             <- The original, immutable data dump.
    â”‚
    â”œâ”€â”€ docs                <- A default Sphinx project; see sphinx-doc.org for details
    â”‚
    â”œâ”€â”€ models              <- Trained and serialized models, model predictions, or model summaries
    â”‚
    â”œâ”€â”€ notebooks           <- Jupyter notebooks. Naming convention is a number (for ordering),
    â”‚                          the creator's initials, and a short `-` delimited description, e.g.
    â”‚                          `1.0-jqp-initial-data-exploration`.
    â”‚
    â”œâ”€â”€ references          <- Data dictionaries, manuals, and all other explanatory materials.
    â”‚
    â”œâ”€â”€ reports             <- Generated analysis as HTML, PDF, LaTeX, etc.
    â”‚Â Â  â””â”€â”€ figures         <- Generated graphics and figures to be used in reporting
    â”‚
    â”œâ”€â”€ requirements.txt    <- The requirements file for reproducing the analysis environment, e.g.
    â”‚                          generated with `pip freeze > requirements.txt`
    â”‚
    â”œâ”€â”€ .env.example        <- A dotenv file template that stores all the credentials and API
    â”‚                         keys in one place; needs to be renamed to .env
    â”‚
    â”œâ”€â”€ docker-compose.yaml <- Defines and runs our multicontainer Docker application
    â”‚
    â”œâ”€â”€ insert_data.py      <- Settles the Postgres database with your data
    â”‚
    â”œâ”€â”€ setup.py            <- Makes project pip installable (pip install -e .) so src can be imported
    â”œâ”€â”€ src                 <- Source code for use in this project.
    â”‚Â Â  â”œâ”€â”€ __init__.py     <- Makes src a Python module
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ data            <- Scripts to download or generate data
    â”‚Â Â  â”‚Â Â  â””â”€â”€ make_dataset.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ features        <- Scripts to turn raw data into features for modeling
    â”‚Â Â  â”‚Â Â  â””â”€â”€ build_features.py
    â”‚   â”‚
    â”‚Â Â  â”œâ”€â”€ models          <- Scripts to train models and then use trained models to make
    â”‚   â”‚   â”‚                  predictions
    â”‚Â Â  â”‚Â Â  â”œâ”€â”€ predict_model.py
    â”‚Â Â  â”‚Â Â  â””â”€â”€ train_model.py
    â”‚   â”‚
    â”‚Â Â  â””â”€â”€ visualization   <- Scripts to create exploratory and results oriented visualizations
    â”‚Â Â      â””â”€â”€ visualize.py
    â”‚
    â””â”€â”€ tox.ini             <- tox file with settings for running tox; see tox.readthedocs.io
